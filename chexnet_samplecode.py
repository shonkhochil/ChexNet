# -*- coding: utf-8 -*-
"""ChexNet-SampleCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jr-4QgeZ4GHSOuYZH1qv7K2tx1IEZZgY
"""

# Commented out IPython magic to ensure Python compatibility.
## Step by step:
# 1. Google Colab - Mount Google Drive
# 2. Load data from Kaggle, Unzip it
# 3. Preprocess image
# 4. Load dense net weights
# 5. Train/Test Classification, Metrics

# ==========================================
# 1. Environment & Data Access Setup
# ==========================================
from google.colab import drive
import os
import re
import numpy as np
import pathlib
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, applications
from sklearn.model_selection import train_test_split

# Mount Drive and set directory
drive.mount("/content/gdrive", force_remount=True)

# Placeholder: Students should define their own working directory path here
WORKING_DIR = "/content/gdrive/MyDrive/path_to_your_project/"
DATASET_PATH = os.path.join(WORKING_DIR, "chest_xray/")

os.makedirs(WORKING_DIR, exist_ok=True)
# %cd $WORKING_DIR

# Set Kaggle credentials environment variable if needed
os.environ['KAGGLE_CONFIG_DIR'] = WORKING_DIR

# --- Block Description: Connects Google Colab to Google Drive and sets up the
# environment for dataset management and model persistence. ---

# ==========================================
# 2. Data Preparation & Splitting
# ==========================================
AUTOTUNE = tf.data.experimental.AUTOTUNE
IMAGE_SIZE = [180, 180]
BATCH_SIZE = 32
EPOCHS = 5

# Collect all filenames from the dataset structure
# Standard NIH/Kaggle structure: chest_xray/train/CLASS_NAME/image.jpeg
filenames = tf.io.gfile.glob(os.path.join(DATASET_PATH, 'train/*/*'))
filenames.extend(tf.io.gfile.glob(os.path.join(DATASET_PATH, 'val/*/*')))

# Split data 80/20 into Training and Validation sets
train_filenames, val_filenames = train_test_split(filenames, test_size=0.2)

# Count classes for weighting (Handling data imbalance)
NORMAL = len([f for f in train_filenames if "NORMAL" in f])
PNEUMONIA = len([f for f in train_filenames if "PNEUMONIA" in f])
TRAIN_COUNT = len(train_filenames)
VAL_COUNT = len(val_filenames)

# --- Block Description: Identifies all image files, splits them into Training and
# Validation sets, and counts instances to calculate class weights. ---

# ==========================================
# 3. Preprocessing Functions & TF.Data Pipeline
# ==========================================
def get_label(file_path):
    parts = tf.strings.split(file_path, os.path.sep)
    # The parent directory name is the class label
    return parts[-2] == "PNEUMONIA"

def decode_img(img):
    img = tf.image.decode_jpeg(img, channels=3)
    # Clinical normalization: Centers pixels around 0 with unit variance
    img = tf.image.per_image_standardization(img)
    img = tf.image.convert_image_dtype(img, tf.float32)
    return tf.image.resize(img, IMAGE_SIZE)

def process_path(path):
    label = get_label(path)
    img = tf.io.read_file(path)
    img = decode_img(img)
    return img, label

def configure_for_performance(ds):
    ds = ds.cache()
    ds = ds.shuffle(buffer_size=1000)
    ds = ds.repeat()
    ds = ds.batch(BATCH_SIZE)
    ds = ds.prefetch(buffer_size=AUTOTUNE)
    return ds

# Create TensorFlow Datasets for high-performance I/O
train_ds = tf.data.Dataset.from_tensor_slices(train_filenames).map(process_path, num_parallel_calls=AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices(val_filenames).map(process_path, num_parallel_calls=AUTOTUNE)

train_ds = configure_for_performance(train_ds)
val_ds = configure_for_performance(val_ds)

# --- Block Description: Defines the clinical "Digital Darkroom." It standardizes
# pixel values, resizes images, and optimizes data flow so the GPU stays saturated. ---

# ==========================================
# 4. Class Weighting (Handling Data Imbalance)
# ==========================================
# Math to ensure the model pays equal attention to both Normal and Pneumonia cases
weight_for_0 = (1 / NORMAL) * (TRAIN_COUNT) / 2.0
weight_for_1 = (1 / PNEUMONIA) * (TRAIN_COUNT) / 2.0
class_weight = {0: weight_for_0, 1: weight_for_1}

# --- Block Description: Medical datasets are often imbalanced. This math increases
# the 'importance' of the minority class during training. ---

# ==========================================
# 5. Building the CheXNet Model
# ==========================================
CHEXNET_WEIGHTS = os.path.join(WORKING_DIR, 'chexnet_weights.h5')

with tf.distribute.get_strategy().scope():
    # Initialize DenseNet-121 backbone
    base = applications.DenseNet121(weights=None, include_top=False, input_shape=(180, 180, 3))

    # 1. Reconstruct the 14-output structure to load pre-trained weights if available
    pred_layer = layers.Dense(14, activation='sigmoid', name='pred')(base.output)
    temp_model = models.Model(inputs=base.input, outputs=pred_layer)

    if os.path.exists(CHEXNET_WEIGHTS):
        temp_model.load_weights(CHEXNET_WEIGHTS)
        print("Pre-trained medical weights loaded.")

    # 2. Adapt for Binary Classification (Pneumonia Detection)
    base.trainable = False  # Freeze convolutional layers

    # Add final classification head
    new_pooling = layers.GlobalAveragePooling2D()(base.layers[-3].output)
    final_output = layers.Dense(1, activation='sigmoid')(new_pooling)

    model = models.Model(inputs=base.input, outputs=final_output)

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), 'accuracy']
    )

# --- Block Description: Builds the DenseNet-121 architecture, adapts it for a
# binary classification task, and compiles it with clinical-grade metrics. ---

# ==========================================
# 6. Training & Evaluation
# ==========================================
history = model.fit(
    train_ds,
    steps_per_epoch=TRAIN_COUNT // BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=val_ds,
    validation_steps=VAL_COUNT // BATCH_SIZE,
    class_weight=class_weight
)

# Visualize Training Progress
fig, ax = plt.subplots(1, 4, figsize=[20, 3])
for i, met in enumerate(['loss', 'precision', 'recall', 'accuracy']):
    ax[i].plot(history.history[met], label='train')
    ax[i].plot(history.history['val_' + met], label='val')
    ax[i].set_title(f'Model {met}')
    ax[i].legend()

# Export model
model.save('ChexNet_Final_Model.keras')
print("Training sequence finished. Model saved.")

# --- Block Description: Runs the training loop, generates learning curves for
# analysis, and saves the final model to your Drive. ---

